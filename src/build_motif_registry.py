"""Motif registry builder.

Iterates over spectrogram .pt files generated by audio2motif.py, extracts
per-channel chirp tensors, feeds them into the motif discovery stub, and stores
results in a simple SQLite registry for later analysis.
"""
from __future__ import annotations

import argparse
import logging
import math
import pickle
import sqlite3
import re
from pathlib import Path
from typing import Iterable, Iterator, Optional

import numpy as np
import torch

import stumpy



# ──────────────────────────────────────────────────────────────────────────────
# SQLite-backed registry
# ──────────────────────────────────────────────────────────────────────────────


class MotifRegistry:
    REQUIRED_META_FIELDS = {
        "sr": int,
        "sample_rate": int,
        "hop_length": int,
        "step_size": int,
        "frame_step": float,
        "n_fft": int,
        "n_mels": int,
        "channels": int,
    }

    def __init__(self, path: Path) -> None:
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.connection = sqlite3.connect(self.path)
        self._ensure_schema()
        self._registry_meta: Optional[dict[str, int | float]] = None

    def _ensure_schema(self) -> None:
        cursor = self.connection.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS motifs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_dataset_id TEXT NOT NULL,
                dim_index INTEGER NOT NULL,
                length INTEGER NOT NULL,
                pattern BLOB NOT NULL,
                created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
            )
            """
        )
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS motif_matches (
                motif_id INTEGER NOT NULL,
                dataset_id TEXT NOT NULL,
                seed_index INTEGER NOT NULL,
                seed_distance REAL NOT NULL,
                starts BLOB NOT NULL,
                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (motif_id, dataset_id),
                FOREIGN KEY (motif_id) REFERENCES motifs(id)
            )
            """
        )
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS channels (
                dataset_id TEXT PRIMARY KEY,
                file_path TEXT NOT NULL,
                channel_index INTEGER NOT NULL,
                split TEXT NOT NULL,
                bird TEXT,
                column_map BLOB NOT NULL,
                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
            )
            """
        )
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS registry_meta (
                key TEXT PRIMARY KEY,
                value BLOB NOT NULL,
                updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
            )
            """
        )
        self.connection.commit()

    def _load_registry_meta(self) -> dict[str, int | float]:
        if self._registry_meta is not None:
            return self._registry_meta
        cursor = self.connection.cursor()
        cursor.execute("SELECT key, value FROM registry_meta")
        meta: dict[str, int | float] = {}
        for key, blob in cursor.fetchall():
            try:
                meta[key] = pickle.loads(blob)
            except Exception:
                continue
        self._registry_meta = meta
        return meta

    def _normalize_meta(self, meta: Optional[dict]) -> Optional[dict[str, int | float]]:
        if not meta:
            return None
        normalized: dict[str, int | float] = {}
        for key, caster in self.REQUIRED_META_FIELDS.items():
            value = meta.get(key)
            if value is None:
                return None
            try:
                normalized[key] = caster(value)
            except (TypeError, ValueError):
                return None
        return normalized

    def _store_registry_meta(self, meta: dict[str, int | float]) -> None:
        cursor = self.connection.cursor()
        for key, value in meta.items():
            cursor.execute(
                """
                INSERT INTO registry_meta(key, value)
                VALUES(?,?)
                ON CONFLICT(key) DO UPDATE SET
                    value=excluded.value,
                    updated_at=CURRENT_TIMESTAMP
                """,
                (key, pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)),
            )
        self.connection.commit()
        self._registry_meta = dict(meta)

    def ensure_metadata(self, meta: Optional[dict]) -> bool:
        normalized = self._normalize_meta(meta)
        if normalized is None:
            logging.warning(
                "PT metadata missing required fields (%s); skipping file.",
                ", ".join(self.REQUIRED_META_FIELDS),
            )
            return False
        current = self._load_registry_meta()
        if not current:
            self._store_registry_meta(normalized)
            return True
        for key, expected_value in current.items():
            candidate = normalized.get(key)
            if candidate is None:
                logging.warning("Metadata field %s missing; skipping file.", key)
                return False
            if isinstance(expected_value, float):
                if not math.isclose(float(candidate), float(expected_value), rel_tol=1e-9, abs_tol=1e-12):
                    logging.warning(
                        "Metadata mismatch (%s): expected %s, found %s. Skipping.",
                        key,
                        expected_value,
                        candidate,
                    )
                    return False
            else:
                if int(candidate) != int(expected_value):
                    logging.warning(
                        "Metadata mismatch (%s): expected %s, found %s. Skipping.",
                        key,
                        expected_value,
                        candidate,
                    )
                    return False
        return True

    def load_motifs(self) -> list[dict]:
        cursor = self.connection.cursor()
        cursor.execute("SELECT id, pattern_dataset_id, dim_index, length, pattern FROM motifs ORDER BY id ASC")
        motifs: list[dict] = []
        for motif_id, pattern_dataset_id, dim_index, length, pattern_blob in cursor.fetchall():
            pattern = pickle.loads(pattern_blob)
            motif = {
                "motif_id": int(motif_id),
                "pattern_dataset_id": pattern_dataset_id,
                "dim_index": int(dim_index),
                "length": int(length),
                "pattern": pattern,
                "matches": {},
            }
            motifs.append(motif)
        return motifs

    def store_channel_metadata(
        self,
        dataset_id: str,
        file_path: Path,
        channel_index: int,
        split: str,
        column_map: np.ndarray,
        bird: Optional[str],
    ) -> None:
        cursor = self.connection.cursor()
        cursor.execute(
            """
            INSERT INTO channels(dataset_id, file_path, channel_index, split, bird, column_map)
            VALUES(?,?,?,?,?,?)
            ON CONFLICT(dataset_id) DO UPDATE SET
                file_path=excluded.file_path,
                channel_index=excluded.channel_index,
                split=excluded.split,
                bird=excluded.bird,
                column_map=excluded.column_map,
                updated_at=CURRENT_TIMESTAMP
            """,
            (
                dataset_id,
                str(file_path),
                int(channel_index),
                split,
                bird,
                pickle.dumps(np.asarray(column_map, dtype=np.int64), protocol=pickle.HIGHEST_PROTOCOL),
            ),
        )
        self.connection.commit()

    def sync_motifs(self, motifs: Optional[list[dict]], dataset_id: Optional[str] = None) -> None:
        cursor = self.connection.cursor()
        if dataset_id is not None:
            cursor.execute("DELETE FROM motif_matches WHERE dataset_id=?", (dataset_id,))
        if not motifs:
            self.connection.commit()
            return
        for motif in motifs:
            pattern_blob = pickle.dumps(np.asarray(motif["pattern"]), protocol=pickle.HIGHEST_PROTOCOL)
            motif_id = motif.get("motif_id")
            if motif_id is None:
                cursor.execute(
                    """
                    INSERT INTO motifs(pattern_dataset_id, dim_index, length, pattern)
                    VALUES(?,?,?,?)
                    """,
                    (
                        motif.get("pattern_dataset_id", ""),
                        int(motif.get("dim_index", 0)),
                        int(motif.get("length", 0)),
                        pattern_blob,
                    ),
                )
                motif_id = cursor.lastrowid
                motif["motif_id"] = int(motif_id)
            else:
                cursor.execute(
                    """
                    UPDATE motifs
                    SET pattern_dataset_id=?, dim_index=?, length=?, pattern=?, updated_at=CURRENT_TIMESTAMP
                    WHERE id=?
                    """,
                    (
                        motif.get("pattern_dataset_id", ""),
                        int(motif.get("dim_index", 0)),
                        int(motif.get("length", 0)),
                        pattern_blob,
                        int(motif_id),
                    ),
                )

        for motif in motifs:
            motif_id = motif.get("motif_id")
            if motif_id is None:
                continue
            for ds_id, match in motif.get("matches", {}).items():
                if dataset_id is not None and ds_id != dataset_id:
                    continue
                starts_blob = pickle.dumps(list(match.get("starts", [])), protocol=pickle.HIGHEST_PROTOCOL)
                cursor.execute(
                    """
                    INSERT INTO motif_matches(motif_id, dataset_id, seed_index, seed_distance, starts)
                    VALUES(?,?,?,?,?)
                    ON CONFLICT(motif_id, dataset_id) DO UPDATE SET
                        seed_index=excluded.seed_index,
                        seed_distance=excluded.seed_distance,
                        starts=excluded.starts,
                        updated_at=CURRENT_TIMESTAMP
                    """,
                    (
                        int(motif_id),
                        ds_id,
                        int(match.get("seed_index", 0)),
                        float(match.get("seed_distance", 0.0)),
                        starts_blob,
                    ),
                )
        self.connection.commit()


# ──────────────────────────────────────────────────────────────────────────────
# Motif discovery stub
# ──────────────────────────────────────────────────────────────────────────────

def print_motif(mot, indent=0, max_array=10):
    """
    Generic pretty-printer for a motif dictionary.
    - No hard-coded keys.
    - Handles arrays, lists, numbers, strings, nested dicts.
    - Truncates long arrays for readability.
    """
    pad = " " * indent
    print("--------------")

    if isinstance(mot, dict):
        for key in sorted(mot.keys()):
            val = mot[key]
            print(f"{pad}{key}: ", end="")

            # --- Recursively print nested dicts ---
            if isinstance(val, dict):
                print()
                print_motif(val, indent + 2, max_array)
                continue

            # --- Arrays (NumPy or lists) ---
            if isinstance(val, (list, tuple)):
                if len(val) > max_array:
                    head = ", ".join(str(v) for v in val[:max_array])
                    print(f"[{head}, ...]  (len={len(val)})")
                else:
                    print(val)
                continue

            # NumPy array
            if hasattr(val, "shape") and hasattr(val, "__array__"):
                arr = np.asarray(val)
                if arr.size > max_array:
                    flat = arr.flat[:max_array]
                    head = ", ".join(str(v) for v in flat)
                    print(f"[{head}, ...]  (shape={arr.shape})")
                else:
                    print(arr)
                continue

            # --- Basic scalar values ---
            print(val)


def _cluster_non_overlapping(starts, m):
    starts = np.sort(np.asarray(starts, int))
    kept = []
    for s in starts:
        if all(abs(s - k) >= m for k in kept):
            kept.append(int(s))
    return kept


def find_motifs(
    T,
    m,
    n_new_motifs=3,
    max_seed_distance=3.5,
    dataset_id="T",
    motifs=None,
):
    T = np.asarray(T, float)

    # Ensure 2D: (d,n)
    if T.ndim == 1:
        T = T.reshape(-1, 1)

    d, n = T.shape
    Tc = T.copy()
    if motifs is None:
        motifs = []

    # Small internal helper just for matching + masking
    def _match_pattern(pattern, length, dim_index):
        series = Tc[dim_index, :]
        matches = stumpy.match(pattern, series , max_distance=max_seed_distance)
        if matches.size == 0:
            return None

        starts = _cluster_non_overlapping(matches[:, 1].astype(int), length)
        if not starts:
            return None

        seed_index = starts[0]
        seed_distance = float(matches[0, 0])

        # Mask out all matched regions so later motifs can't reuse them
        for s in starts:
            Tc[:, s : s + length] = np.nan

        return {
            "seed_index": seed_index,
            "seed_distance": seed_distance,
            "starts": starts,
        }

    # ---------------------------------------------------------
    # 1) Use existing motifs: match their patterns in this dataset
    # ---------------------------------------------------------
    for mot in motifs:
        dim_idx = int(mot.get("dim_index", 0))
        if dim_idx < 0 or dim_idx >= d:
            continue  # this dataset doesn't have that dimension


        pat = np.asarray(mot["pattern"], float)
        L = int(mot["length"])

        res = _match_pattern(pat, L, dim_idx)
        if res is None:
            continue

        matches_dict = mot.setdefault("matches", {})
        matches_dict[dataset_id] = res

    # ---------------------------------------------------------
    # 2) Discover new motifs of length m for this dataset
    # ---------------------------------------------------------
    for _ in range(n_new_motifs):
        mps, indices = stumpy.mstump(Tc, m)

        if not np.isfinite(mps).any():
            break

        # Find the motif in the joint (3D) profile
        motifs_idx = np.nanargmin(mps)
        nn_idx    = indices[np.arange(d), motifs_idx]

        # --- best index PER k-row (required by mdl!) ---
        motifs_idx = np.nanargmin(mps, axis=1)         # shape (d,)
        nn_idx = indices[np.arange(d), motifs_idx]     # shape (d,)


        # MDL to choose best k and its subspace
        mdls, subspaces = stumpy.mdl(Tc, m, motifs_idx, nn_idx)

        K = min(3, d)
        k_best = int(np.argmin(mdls[:K]))

        motif_idx = int(motifs_idx[k_best])
        seed_dist = float(mps[k_best, motif_idx])

        # Stopping condition based on joint distance
        if max_seed_distance is not None and seed_dist > max_seed_distance:
            break

        # Choose the most important specific dimension driving this motif
        dims_best = np.asarray(subspaces[k_best], int)
        if dims_best.size == 0:
            break
        dim_index = int(dims_best[0])

        # Use original T (not Tc) to store canonical 1D pattern
        if motif_idx + m > n:
            break
        Q = T[ dim_index, motif_idx : motif_idx + m]
        if np.isnan(Q).any():
            break

        res = _match_pattern(Q, m, dim_index)
        if res is None:
            break

        motif = {
            "pattern_dataset_id": dataset_id,
            "dim_index": dim_index,
            "pattern": Q.copy(),
            "length": int(m),
            "matches": {
                dataset_id: res,
            },
        }
        motifs.append(motif)

    return motifs


# ──────────────────────────────────────────────────────────────────────────────
# Helpers
# ──────────────────────────────────────────────────────────────────────────────


def _iter_pt_files(root: Path, split: str) -> Iterator[Path]:
    base = Path(root) / split
    if not base.exists():
        raise FileNotFoundError(f"split directory does not exist: {base}")
    for path in sorted(base.glob("**/*.pt")):
        if path.is_file():
            yield path


def _to_numpy(array: object) -> np.ndarray:
    if isinstance(array, torch.Tensor):
        return array.detach().cpu().numpy()
    return np.asarray(array)


def _channel_intervals(payload: dict, channel_index: int) -> np.ndarray:
    intervals_raw = payload.get("chirp_intervals")
    if intervals_raw is None:
        return np.empty((0, 2), dtype=np.int64)
    intervals = _to_numpy(intervals_raw)
    lengths = payload.get("chirp_lengths")
    length_arr = _to_numpy(lengths) if lengths is not None else None

    if intervals.ndim == 3:
        channel = intervals[channel_index]
        take = int(length_arr[channel_index]) if length_arr is not None else channel.shape[0]
        channel = channel[:take]
    else:
        channel = intervals

    channel = np.asarray(channel, dtype=np.int64).reshape(-1, 2)
    if channel.size == 0:
        return np.empty((0, 2), dtype=np.int64)
    valid = (channel[:, 0] >= 0) & (channel[:, 1] > channel[:, 0])
    return channel[valid]


def _build_chirp_tensor(channel_spec: np.ndarray, intervals: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    if intervals.size == 0:
        return np.empty((channel_spec.shape[0], 0), dtype=channel_spec.dtype), np.empty((0,), dtype=np.int64)

    segments: list[np.ndarray] = []
    column_map: list[int] = []
    for idx, (start, end) in enumerate(intervals):
        start_i = int(start)
        end_i = int(end)
        if end_i <= start_i:
            continue
        chirp = channel_spec[:, start_i:end_i]
        if chirp.size == 0:
            continue
        segments.append(chirp)
        column_map.extend(range(start_i, end_i))
        if idx < intervals.shape[0] - 1:
            nan_col = np.full((channel_spec.shape[0], 1), np.nan, dtype=channel_spec.dtype)
            segments.append(nan_col)
            column_map.append(-1)

    if not segments:
        return np.empty((channel_spec.shape[0], 0), dtype=channel_spec.dtype), np.empty((0,), dtype=np.int64)

    tensor = np.hstack(segments)
    return tensor, np.asarray(column_map, dtype=np.int64)


def _normalize_by_row(spectrogram: np.ndarray) -> np.ndarray:
    normalized = spectrogram.astype(np.float32, copy=True)
    for row_idx in range(normalized.shape[0]):
        row = normalized[row_idx]
        mask = np.isfinite(row)
        if not mask.any():
            continue
        mean = float(np.mean(row[mask]))
        std = float(np.std(row[mask]))
        if std < 1e-8:
            std = 1.0
        normalized[row_idx, mask] = (row[mask] - mean) / std
    return normalized


def _load_payload(path: Path) -> dict:
    payload = torch.load(path, map_location="cpu")
    if isinstance(payload, dict):
        return payload
    raise ValueError(f"{path}: expected dict payload, found {type(payload)}")


_BIRD_PATTERN = re.compile(r"^(?P<timestamp>[^._]+)_(?P<bird0>[^._]+)_(?P<bird1>[^._]+)")


def _parse_birds_from_stem(stem: str) -> tuple[Optional[str], Optional[str]]:
    match = _BIRD_PATTERN.match(stem)
    if not match:
        return None, None
    return match.group("bird0"), match.group("bird1")


def _resolve_bird_label(file_path: Path, channel_index: int) -> str:
    bird0, bird1 = _parse_birds_from_stem(file_path.stem)
    if channel_index == 0:
        return bird0 or "unknown-bird0"
    if channel_index == 1:
        return bird1 or "unknown-bird1"
    if bird0 == bird1 and bird0:
        return bird0
    birds = ", ".join([bird for bird in (bird0, bird1) if bird])
    return birds or f"channel{channel_index}"


def _build_dataset_id(file_path: Path, channel_index: int) -> str:
    return f"{file_path}|ch{channel_index}"


# ──────────────────────────────────────────────────────────────────────────────
# Main pipeline
# ──────────────────────────────────────────────────────────────────────────────


def process_pt_file(
    file_path: Path,
    split: str,
    registry: MotifRegistry,
    motif_length: int,
    n_new_motifs: int,
    max_seed_distance: float,
    motifs_state: Optional[list[dict]],
) -> Optional[list[dict]]:
    payload = _load_payload(file_path)
    meta = payload.get("meta")
    if not registry.ensure_metadata(meta):
        logging.warning("Skipping %s: incompatible metadata.", file_path)
        return motifs_state
    spectrogram = _to_numpy(payload.get("s"))
    if spectrogram.ndim != 3:
        raise ValueError(f"{file_path}: expected spectrogram stack (channels, freqs, time).")

    channels = spectrogram.shape[0]
    for channel_index in range(channels):
        channel_spec = spectrogram[channel_index]
        intervals = _channel_intervals(payload, channel_index)
        if intervals.size == 0:
            logging.info(
                "Skipping channel %d for %s (no chirp intervals).",
                channel_index,
                file_path.name,
            )
            continue

        chirp_tensor, column_map = _build_chirp_tensor(channel_spec, intervals)
        dataset_id = _build_dataset_id(file_path, channel_index)
        bird_label = _resolve_bird_label(file_path, channel_index)
        registry.store_channel_metadata(dataset_id, file_path, channel_index, split, column_map, bird_label)
        if chirp_tensor.size == 0:
            logging.info(
                "Skipping channel %d for %s (empty chirp tensor).",
                channel_index,
                file_path.name,
            )
            continue

        normalized = _normalize_by_row(chirp_tensor)
        motifs = find_motifs(
            normalized,
            motif_length,
            n_new_motifs=n_new_motifs,
            max_seed_distance=max_seed_distance,
            dataset_id=dataset_id,
            motifs=motifs_state,
        )
        motifs_state = motifs or []
        registry.sync_motifs(motifs_state, dataset_id=dataset_id)
        logging.info(
            "Processed %s channel %d (chirps=%d, cols=%d, dataset_id=%s)",
            file_path.name,
            channel_index,
            intervals.shape[0],
            normalized.shape[1],
            dataset_id,
        )

    return motifs_state


def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build or update motif registry from spectrogram tensors.")
    parser.add_argument("--data_dir", type=Path, required=True, help="Root directory containing train/validation splits.")
    parser.add_argument(
        "--split",
        type=str,
        choices=("train", "validation"),
        required=True,
        help="Split to process.",
    )
    parser.add_argument("--registry", type=Path, default=Path("motif_registry.sqlite"), help="Registry database path.")
    parser.add_argument("--motif_length", type=int, default=128, help="Motif length (time bins).")
    parser.add_argument("--n_new_motifs", type=int, default=3, help="Maximum motifs to add per channel.")
    parser.add_argument(
        "--max_seed_distance",
        type=float,
        default=3.5,
        help="Maximum distance between motif seeds when growing the registry.",
    )
    return parser.parse_args(argv)


def main(argv: Optional[Iterable[str]] = None) -> None:
    args = parse_args(argv)
    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
    registry = MotifRegistry(args.registry)

    motif_state = registry.load_motifs()
    if not motif_state:
        motif_state = None

    for pt_file in _iter_pt_files(args.data_dir, args.split):
        try:
            motif_state = process_pt_file(
                pt_file,
                args.split,
                registry,
                motif_length=args.motif_length,
                n_new_motifs=args.n_new_motifs,
                max_seed_distance=args.max_seed_distance,
                motifs_state=motif_state,
            )
        except Exception as exc:
            logging.exception("Failed processing %s: %s", pt_file, exc)


if __name__ == "__main__":
    main()
