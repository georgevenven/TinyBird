import argparse 
import json
from model import TinyBird
from data_loader import SpectogramDataset
from utils import load_model_from_checkpoint
import torch
import umap
import numpy as np

def main(args):

    # Load syllable labels if provided
    syllable_data_list = None
    if args["syllable_labels"]:
        with open(args["syllable_labels"], 'r') as f:
            syllable_data_list = json.load(f)
        # Convert list to dictionary keyed by filename for easier lookup
        syllable_data_dict = {}
        if isinstance(syllable_data_list, list):
            for record in syllable_data_list:
                if "filename" in record:
                    syllable_data_dict[record["filename"]] = record
        else:
            # Handle case where it's a single record (not a list)
            syllable_data_dict["single_record"] = syllable_data_list

    # Load model and config from checkpoint
    model, config = load_model_from_checkpoint(
        run_dir=args["run_dir"],
        checkpoint_file=args["checkpoint"]
    )
    # Create embedding dataset using config parameters
    embedding_dataset = SpectogramDataset(
        dir=args["spec_dir"],
        n_mels=config["mels"],
        n_timebins=config["num_timebins"],
        pad_crop=False
    )

    def ms_to_timebin(ms):
        """Convert milliseconds to timebin index"""
        # Hardcoded values: 32kHz sample rate, 256 timebins per spec
        sample_rate = 32000
        timebins_per_spec = 256
        # For spectrograms: timebin = ms * sample_rate / (1000 * hop_length)
        # Assuming hop_length = sample_rate / timebins_per_spec for this duration
        hop_length = sample_rate // timebins_per_spec  # ~125 samples per timebin
        return int(ms * sample_rate / (1000 * hop_length))
    
    def get_syllable_colors(syllable_data, total_timebins_processed, spec_timebins):
        """Determine syllable colors for each timebin in the current spectrogram"""
        if not syllable_data:
            return np.zeros(spec_timebins)  # Default color (0) if no syllable data
        
        syllable_labels = syllable_data.get("syllable_labels", {})
        
        colors = np.zeros(spec_timebins)
        
        # First, collect all syllable segments and sort them
        all_segments = []
        for syllable_id, time_ranges in syllable_labels.items():
            syllable_color = int(syllable_id)
            for start_ms, end_ms in time_ranges:
                start_timebin = ms_to_timebin(start_ms * 1000)
                end_timebin = ms_to_timebin(end_ms * 1000)
                all_segments.append((start_timebin, end_timebin, syllable_color))
        
        # Sort segments by start time
        all_segments.sort(key=lambda x: x[0])
        
        # Fill in syllables and gaps between same syllables
        for i, (start_timebin, end_timebin, syllable_color) in enumerate(all_segments):
            # Map to local timebin indices in current spectrogram
            local_start = max(0, start_timebin - total_timebins_processed)
            local_end = min(spec_timebins, end_timebin - total_timebins_processed)
            
            if local_start < spec_timebins and local_end > 0:
                colors[local_start:local_end] = syllable_color
            
            # Check if next segment is the same syllable and fill gap
            if i < len(all_segments) - 1:
                next_start, next_end, next_syllable = all_segments[i + 1]
                if next_syllable == syllable_color:
                    # Fill gap between current end and next start
                    gap_start = max(0, end_timebin - total_timebins_processed)
                    gap_end = min(spec_timebins, next_start - total_timebins_processed)
                    
                    if gap_start < spec_timebins and gap_end > 0 and gap_start < gap_end:
                        colors[gap_start:gap_end] = syllable_color
        
        return colors

    latent_list = []
    color_list = []
    total_timebins = 0
    i = 0
    while i < len(embedding_dataset) and total_timebins < args["num_timebins"]:
        spec, filename = embedding_dataset[i]
        
        # Check if file should be processed when syllable labeling is enabled
        current_syllable_data = None
        if args["syllable_labels"]:
            # Try to find syllable data for this file
            # The filename might need .wav extension to match JSON records
            potential_names = [filename, filename + ".wav", "bird_" + filename + ".wav"]
            found_file = False
            for name in potential_names:
                if name in syllable_data_dict:
                    current_syllable_data = syllable_data_dict[name]
                    found_file = True
                    break
            
            # Skip this file if not found in JSON when syllable labeling is activated
            if not found_file:
                i += 1
                continue
        
        patch_height, patch_width, mels, num_timebins = config["patch_height"], config["patch_width"], config["mels"], config["num_timebins"]
        spec = spec[:,:mels,:num_timebins]

        spec_timebins = spec.shape[-1]
        spec_mels = spec.shape[-2]

        remainder = spec_timebins % patch_width
        if remainder != 0: spec = spec[:,:,:spec_timebins-remainder]
        spec_timebins = spec.shape[-1]
        spec_mels = spec.shape[-2]

        spec = spec.unsqueeze(0) # add a batch dimension
        
        # TODO: temp conversion to float32, should be removed later
        spec = spec.float()
        z = model.forward_encoder_inference(spec)
        B, S, D = z.shape

        H = int(spec_mels / config["patch_height"])
        W = int(spec_timebins / config["patch_width"])

        z_grid = z.reshape(B, H, W, D) 
        z_grid = z_grid.permute(0,2,1,3)
        z_freq_stack = z_grid.reshape(B, W, H * D) 

        # get rid of the batch shape (its 1 anyway)
        z_freq_stack = z_freq_stack.squeeze(0)

        # shape is W, H*D
        # Remove positional encoding bias by subtracting mean across positions
        if args["subtract_pos"]:
            z_mean = z_freq_stack.mean(dim=0, keepdim=True)  # mean across time dimension
            z_centered = z_freq_stack - z_mean
        else:
            z_centered = z_freq_stack

        latent_list.append(z_centered)
        
        # Get syllable colors for this spectrogram
        syllable_colors = get_syllable_colors(current_syllable_data, total_timebins, spec_timebins)
        
        # Map colors to patches - each patch covers patch_width timebins
        patch_colors = []
        for patch_idx in range(W):  # W is number of patches in time dimension
            start_timebin = patch_idx * patch_width
            end_timebin = min((patch_idx + 1) * patch_width, spec_timebins)
            # Use the most common syllable in this patch (mode)
            patch_syllables = syllable_colors[start_timebin:end_timebin]
            if len(patch_syllables) > 0:
                # Get the most frequent syllable in this patch
                unique_vals, counts = np.unique(patch_syllables, return_counts=True)
                most_common_syllable = unique_vals[np.argmax(counts)]
                patch_colors.append(most_common_syllable)
            else:
                patch_colors.append(0)  # Default to background
        
        color_list.append(np.array(patch_colors))

        total_timebins += spec_timebins
        i += 1

    Z = torch.cat(latent_list, dim=0)
    colors = np.concatenate(color_list, axis=0)
    
    reducer_enc = umap.UMAP(n_components=2, metric='cosine', n_jobs=-1, low_memory=True, n_neighbors=50)
    emb_enc = reducer_enc.fit_transform(Z.detach().cpu().numpy())

    # Plot the embedding
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(10, 8))
    
    if args["syllable_labels"] and len(np.unique(colors)) > 1:
        # Color by syllable labels
        unique_syllables = np.unique(colors)
        cmap = plt.cm.tab10  # Use a discrete colormap
        scatter = plt.scatter(emb_enc[:, 0], emb_enc[:, 1], c=colors, cmap=cmap, alpha=0.1)
        
        # Add colorbar with syllable labels
        cbar = plt.colorbar(scatter)
        cbar.set_label('Syllable Label')
        cbar.set_ticks(unique_syllables)
        cbar.set_ticklabels([f'Syllable {int(s)}' if s > 0 else 'Background' for s in unique_syllables])
        
        plt.title('UMAP Embedding of Spectrogram Patches (Colored by Syllable Labels)')
    else:
        # Default coloring when no syllable data is provided
        plt.scatter(emb_enc[:, 0], emb_enc[:, 1], alpha=0.7)
        plt.title('UMAP Embedding of Spectrogram Patches')
    
    plt.xlabel('UMAP 1')
    plt.ylabel('UMAP 2')
    plt.grid(True, alpha=0.3)
    plt.show()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="plotting embedding args")
    parser.add_argument("--num_timebins", type=int, required=True, help="Number of time bins")
    parser.add_argument("--run_dir", type=str, required=True, help="Run directory path")
    parser.add_argument("--checkpoint", type=str, default=None, help="Checkpoint file (optional)")
    parser.add_argument("--spec_dir", type=str, required=True, help="Directory of specs to plot the embedding of")
    parser.add_argument("--subtract_pos", action="store_true", help="Subtract positional encoding bias")
    parser.add_argument("--syllable_labels", type=str, default=None, help="Path to JSON file with syllable label information")
    args = parser.parse_args()
    main(vars(args))
